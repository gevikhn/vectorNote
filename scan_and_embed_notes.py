import os
import re
import uuid
import hashlib
from pathlib import Path
from tqdm import tqdm
from sentence_transformers import SentenceTransformer
from qdrant_client import QdrantClient
from qdrant_client.models import PointStruct, Distance, VectorParams

# === é…ç½®é¡¹ ===
ROOT_DIR = Path("D:/Notes")  # <-- âš ï¸ ä¿®æ”¹ä¸ºä½ çš„ Obsidian æ ¹ç›®å½•è·¯å¾„
EXTENSIONS = [".md"]
CHUNK_SIZE = 300
COLLECTION_NAME = "obsidian_notes"
MODEL_NAME = "BAAI/bge-small-zh"
VECTOR_DIM = 512  # ä¿®æ”¹ä¸ºæ¨¡å‹çš„å®é™…è¾“å‡ºç»´åº¦

# === åŠ è½½æ¨¡å‹ & å¯åŠ¨ Qdrant ===
print("ğŸ” åŠ è½½æ¨¡å‹ä¸æ•°æ®åº“...")
model = SentenceTransformer(MODEL_NAME)
client = QdrantClient(path="./qdrant_data")
client.recreate_collection(
    collection_name=COLLECTION_NAME,
    vectors_config=VectorParams(size=VECTOR_DIM, distance=Distance.COSINE),
)

# === Markdown åˆ†æ®µå‡½æ•° ===
def split_markdown_chunks(markdown_text: str, chunk_size: int = CHUNK_SIZE):
    blocks = []
    current_header = ""
    current_content = []
    lines = markdown_text.splitlines()

    for line in lines:
        if re.match(r'^\s*#{1,6}\s', line):
            if current_header or current_content:
                blocks.append((current_header, "\n".join(current_content).strip()))
            current_header = line.strip()
            current_content = []
        else:
            current_content.append(line)

    if current_header or current_content:
        blocks.append((current_header, "\n".join(current_content).strip()))

    final_chunks = []
    for header, content in blocks:
        if len(content) <= chunk_size:
            final_chunks.append((header, content))
        else:
            parts = re.split(r'(?<=[ã€‚ï¼ï¼Ÿ])\s*', content)
            buffer = ""
            for p in parts:
                if not p.strip():
                    continue
                if len(buffer) + len(p) < chunk_size:
                    buffer += p
                else:
                    final_chunks.append((header, buffer.strip()))
                    buffer = p
            if buffer:
                final_chunks.append((header, buffer.strip()))

    return final_chunks

# === éå†ç¬”è®°å¹¶å†™å…¥å‘é‡æ•°æ®åº“ ===
all_points = []
file_count = 0

print("ğŸ“ æ­£åœ¨æ‰«æå¹¶å¤„ç† Markdown æ–‡ä»¶...")
for path in tqdm(list(ROOT_DIR.rglob("*"))):
    if not path.is_file() or path.suffix.lower() not in EXTENSIONS:
        continue

    try:
        text = path.read_text(encoding="utf-8")
    except Exception:
        continue

    chunks = split_markdown_chunks(text)
    if not chunks:
        continue

    sentences = [f"{title}\n{content}" if title else content for title, content in chunks]
    vectors = model.encode(sentences, show_progress_bar=False)

    for i, vec in enumerate(vectors):
        # ä½¿ç”¨ uuid5 åŸºäº SHA1 å“ˆå¸Œç”Ÿæˆç¡®å®šæ€§ UUID
        namespace = uuid.NAMESPACE_DNS
        uid = uuid.uuid5(namespace, f"{path}-{i}")
        all_points.append(PointStruct(
            id=str(uid),  # å°† UUID è½¬æ¢ä¸ºå­—ç¬¦ä¸²
            vector=vec.tolist(),
            payload={
                "text": sentences[i],
                "source": str(path)
            }
        ))

    file_count += 1
    if len(all_points) >= 128:
        client.upsert(collection_name=COLLECTION_NAME, points=all_points)
        all_points = []

if all_points:
    client.upsert(collection_name=COLLECTION_NAME, points=all_points)

print(f"âœ… å®Œæˆï¼šå…±å¤„ç† {file_count} ä¸ªæ–‡ä»¶ï¼Œå‘é‡æ•°æ®å·²å†™å…¥ Qdrant æ•°æ®åº“ã€‚")