import os
import re
import uuid
import hashlib
from pathlib import Path
from tqdm import tqdm
from sentence_transformers import SentenceTransformer
from qdrant_client import QdrantClient
from qdrant_client.models import PointStruct, Distance, VectorParams

# === é…ç½®é¡¹ ===
ROOT_DIR = Path("D:/Notes")  # <-- âš ï¸ ä¿®æ”¹ä¸ºä½ çš„ Obsidian æ ¹ç›®å½•è·¯å¾„
EXTENSIONS = [".md"]
CHUNK_SIZE = 300
COLLECTION_NAME = "obsidian_notes"
MODEL_NAME = "BAAI/bge-small-zh"
VECTOR_DIM = 512  # ä¿®æ”¹ä¸ºæ¨¡å‹çš„å®é™…è¾“å‡ºç»´åº¦

# === åŠ è½½æ¨¡å‹ & å¯åŠ¨ Qdrant ===
print("ğŸ” åŠ è½½æ¨¡å‹ä¸æ•°æ®åº“...")
model = SentenceTransformer(MODEL_NAME)
client = QdrantClient(path="./qdrant_data")
client.recreate_collection(
    collection_name=COLLECTION_NAME,
    vectors_config=VectorParams(size=VECTOR_DIM, distance=Distance.COSINE),
)

# === Markdown åˆ†æ®µå‡½æ•° ===
def split_markdown_chunks(markdown_text: str, chunk_size: int = CHUNK_SIZE, overlap: int = 50):
    """
    æ™ºèƒ½åˆ†å‰² Markdown æ–‡æœ¬ä¸ºå¤šä¸ªè¯­ä¹‰å—
    
    ç‰¹ç‚¹:
    1. ä¿ç•™æ–‡æ¡£ç»“æ„å’Œæ ‡é¢˜å±‚çº§
    2. æ™ºèƒ½å¤„ç†ä»£ç å—ã€åˆ—è¡¨ç­‰ç‰¹æ®Šæ ¼å¼
    3. æ·»åŠ ä¸Šä¸‹æ–‡é‡å ä»¥æé«˜æœç´¢è´¨é‡
    4. æ›´å¥½åœ°å¤„ç†ä¸­è‹±æ–‡æ··åˆæ–‡æœ¬
    """
    blocks = []
    current_headers = []  # å­˜å‚¨å½“å‰æ ‡é¢˜å±‚çº§
    current_content = []
    in_code_block = False
    in_list = False
    lines = markdown_text.splitlines()
    
    # ç¬¬ä¸€æ­¥ï¼šæŒ‰ç…§æ–‡æ¡£ç»“æ„åˆ†å—
    for line in lines:
        # å¤„ç†ä»£ç å—
        if line.strip().startswith("```"):
            in_code_block = not in_code_block
            current_content.append(line)
            continue
            
        # åœ¨ä»£ç å—å†…ï¼Œä¸åšç‰¹æ®Šå¤„ç†
        if in_code_block:
            current_content.append(line)
            continue
            
        # å¤„ç†æ ‡é¢˜
        header_match = re.match(r'^(\s*)(#{1,6})\s+(.*?)$', line)
        if header_match:
            # ä¿å­˜ä¹‹å‰çš„å†…å®¹å—
            if current_content:
                header_text = " > ".join(current_headers) if current_headers else ""
                blocks.append((header_text, "\n".join(current_content).strip()))
                current_content = []
            
            # æ›´æ–°å½“å‰æ ‡é¢˜å±‚çº§
            level = len(header_match.group(2))
            title = header_match.group(3).strip()
            
            # æ ¹æ®æ ‡é¢˜çº§åˆ«è°ƒæ•´æ ‡é¢˜å±‚çº§
            current_headers = current_headers[:level-1]  # ç§»é™¤æ›´ä½çº§åˆ«çš„æ ‡é¢˜
            if level == 1:  # å¦‚æœæ˜¯ä¸€çº§æ ‡é¢˜ï¼Œæ¸…ç©ºæ‰€æœ‰æ ‡é¢˜
                current_headers = [title]
            else:
                current_headers.append(title)
                
            continue
            
        # å¤„ç†åˆ—è¡¨é¡¹
        list_match = re.match(r'^\s*[-*+]\s+', line) or re.match(r'^\s*\d+\.\s+', line)
        if list_match:
            in_list = True
        elif in_list and not line.strip():
            in_list = False
            
        # æ·»åŠ å½“å‰è¡Œåˆ°å†…å®¹
        current_content.append(line)
    
    # å¤„ç†æœ€åä¸€ä¸ªå—
    if current_content:
        header_text = " > ".join(current_headers) if current_headers else ""
        blocks.append((header_text, "\n".join(current_content).strip()))
    
    # ç¬¬äºŒæ­¥ï¼šå¤„ç†è¿‡é•¿çš„å†…å®¹å—
    final_chunks = []
    for header, content in blocks:
        if len(content) <= chunk_size:
            final_chunks.append((header, content))
        else:
            # ä¼˜å…ˆæŒ‰æ®µè½åˆ†å‰²
            paragraphs = re.split(r'\n\s*\n', content)
            
            if len(max(paragraphs, key=len)) > chunk_size:
                # å¦‚æœæ®µè½ä»ç„¶å¤ªé•¿ï¼ŒæŒ‰å¥å­åˆ†å‰²
                buffer = ""
                # åŒæ—¶å¤„ç†ä¸­è‹±æ–‡å¥å·
                parts = re.split(r'(?<=[ã€‚ï¼ï¼Ÿ.!?])\s*', content)
                
                for p in parts:
                    if not p.strip():
                        continue
                    
                    if len(buffer) + len(p) < chunk_size:
                        buffer += p
                    else:
                        if buffer:
                            final_chunks.append((header, buffer.strip()))
                        buffer = p
                
                if buffer:  # æ·»åŠ æœ€åä¸€ä¸ªç¼“å†²åŒº
                    final_chunks.append((header, buffer.strip()))
            else:
                # æŒ‰æ®µè½åˆ†å‰²å¹¶æ·»åŠ é‡å 
                buffer = ""
                for p in paragraphs:
                    if len(buffer) + len(p) + 1 < chunk_size:  # +1 for newline
                        buffer += p + "\n\n"
                    else:
                        if buffer:
                            final_chunks.append((header, buffer.strip()))
                        buffer = p + "\n\n"
                
                if buffer:  # æ·»åŠ æœ€åä¸€ä¸ªç¼“å†²åŒº
                    final_chunks.append((header, buffer.strip()))
    
    # ç¬¬ä¸‰æ­¥ï¼šæ·»åŠ ä¸Šä¸‹æ–‡é‡å ï¼ˆå¯é€‰ï¼‰
    if overlap > 0 and len(final_chunks) > 1:
        overlapped_chunks = []
        for i, (header, content) in enumerate(final_chunks):
            if i > 0:
                # ä»å‰ä¸€ä¸ªå—çš„æœ«å°¾æ·»åŠ é‡å å†…å®¹
                prev_content = final_chunks[i-1][1]
                overlap_text = prev_content[-overlap:] if len(prev_content) > overlap else prev_content
                content = overlap_text + "..." + content
            
            if i < len(final_chunks) - 1:
                # ä»åä¸€ä¸ªå—çš„å¼€å¤´æ·»åŠ é‡å å†…å®¹
                next_content = final_chunks[i+1][1]
                overlap_text = next_content[:overlap] if len(next_content) > overlap else next_content
                content = content + "..." + overlap_text
                
            overlapped_chunks.append((header, content))
        return overlapped_chunks
    
    return final_chunks

# === éå†ç¬”è®°å¹¶å†™å…¥å‘é‡æ•°æ®åº“ ===
all_points = []
file_count = 0

print("ğŸ“ æ­£åœ¨æ‰«æå¹¶å¤„ç† Markdown æ–‡ä»¶...")
for path in tqdm(list(ROOT_DIR.rglob("*"))):
    if not path.is_file() or path.suffix.lower() not in EXTENSIONS:
        continue

    try:
        text = path.read_text(encoding="utf-8")
    except Exception:
        continue

    # æå–æ–‡ä»¶åï¼ˆä¸å«æ‰©å±•åï¼‰ä½œä¸ºé¢å¤–çš„æ–‡æœ¬å†…å®¹
    filename = path.stem
    
    # å¤„ç†æ–‡ä»¶åï¼Œå°†é©¼å³°å‘½åã€ä¸‹åˆ’çº¿ã€è¿å­—ç¬¦ç­‰åˆ†å‰²ä¸ºç©ºæ ¼
    # ä¾‹å¦‚: "Windowså¼€å¯bbr" -> "Windows å¼€å¯ bbr"
    # ä¾‹å¦‚: "windows_bbr_config" -> "windows bbr config"
    processed_filename = re.sub(r'([a-z])([A-Z])', r'\1 \2', filename)  # é©¼å³°è½¬ç©ºæ ¼
    processed_filename = re.sub(r'[_\-.]', ' ', processed_filename)     # ä¸‹åˆ’çº¿ã€è¿å­—ç¬¦è½¬ç©ºæ ¼
    
    # å°†æ–‡ä»¶åæ·»åŠ åˆ°æ–‡æœ¬å†…å®¹çš„å¼€å¤´ï¼Œå¢åŠ æƒé‡
    text_with_filename = f"# {processed_filename}\n\n{text}"
    
    chunks = split_markdown_chunks(text_with_filename)
    if not chunks:
        continue

    sentences = [f"{title}\n{content}" if title else content for title, content in chunks]
    vectors = model.encode(sentences, show_progress_bar=False)

    for i, vec in enumerate(vectors):
        # ä½¿ç”¨ uuid5 åŸºäº SHA1 å“ˆå¸Œç”Ÿæˆç¡®å®šæ€§ UUID
        namespace = uuid.NAMESPACE_DNS
        uid = uuid.uuid5(namespace, f"{path}-{i}")
        all_points.append(PointStruct(
            id=str(uid),  # å°† UUID è½¬æ¢ä¸ºå­—ç¬¦ä¸²
            vector=vec.tolist(),
            payload={
                "text": sentences[i],
                "source": str(path),
                "filename": path.name  # æ·»åŠ æ–‡ä»¶ååˆ°payload
            }
        ))

    # é¢å¤–æ·»åŠ ä¸€ä¸ªä»…åŒ…å«æ–‡ä»¶åçš„å‘é‡ç‚¹ï¼Œæé«˜æ–‡ä»¶åæœç´¢çš„å‡†ç¡®æ€§
    filename_vector = model.encode(processed_filename)
    filename_uid = uuid.uuid5(namespace, f"{path}-filename")
    all_points.append(PointStruct(
        id=str(filename_uid),
        vector=filename_vector.tolist(),
        payload={
            "text": f"# {processed_filename}",
            "source": str(path),
            "filename": path.name,
            "is_filename_only": True  # æ ‡è®°è¿™æ˜¯ä¸€ä¸ªä»…åŒ…å«æ–‡ä»¶åçš„å‘é‡ç‚¹
        }
    ))

    file_count += 1
    if len(all_points) >= 128:
        client.upsert(collection_name=COLLECTION_NAME, points=all_points)
        all_points = []

if all_points:
    client.upsert(collection_name=COLLECTION_NAME, points=all_points)

print(f"âœ… å®Œæˆï¼šå…±å¤„ç† {file_count} ä¸ªæ–‡ä»¶ï¼Œå‘é‡æ•°æ®å·²å†™å…¥ Qdrant æ•°æ®åº“ã€‚")