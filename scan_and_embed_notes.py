import os
import re
import uuid
import hashlib
import sys
import torch
from pathlib import Path
from tqdm import tqdm
from sentence_transformers import SentenceTransformer
from qdrant_client import QdrantClient
from qdrant_client.models import PointStruct, Distance, VectorParams

# === é…ç½®é¡¹ ===
ROOT_DIR = Path("D:/Notes")  # <-- âš ï¸ ä¿®æ”¹ä¸ºä½ çš„ Obsidian æ ¹ç›®å½•è·¯å¾„
EXTENSIONS = [".md"]
CHUNK_SIZE = 1024
COLLECTION_NAME = "obsidian_notes"
# å‡çº§åˆ°æ›´å¼ºå¤§çš„æ¨¡å‹
MODEL_NAME = "BAAI/bge-large-zh-noinstruct"  # æˆ–è€… "text2vec-large-chinese"
VECTOR_DIM = 1024  # ä¿®æ”¹ä¸ºæ¨¡å‹çš„å®é™…è¾“å‡ºç»´åº¦

# === æ£€æµ‹CUDAå¯ç”¨æ€§ ===
def check_cuda_availability():
    """æ£€æµ‹æ˜¯å¦æœ‰å¯ç”¨çš„CUDAè®¾å¤‡ï¼Œç‰¹åˆ«é’ˆå¯¹Windowsç¯å¢ƒä¼˜åŒ–"""
    try:
        # å°è¯•ç›´æ¥è·å–CUDAè®¾å¤‡ä¿¡æ¯
        if torch.cuda.is_available():
            device_count = torch.cuda.device_count()
            device_name = torch.cuda.get_device_name(0) if device_count > 0 else "æœªçŸ¥"
            print(f"âœ… æ£€æµ‹åˆ° {device_count} ä¸ªCUDAè®¾å¤‡: {device_name}")
            print(f"âœ… å°†ä½¿ç”¨GPUè¿›è¡ŒåŠ é€Ÿå¤„ç†")
            return "cuda"
        
        # å¦‚æœä¸Šé¢çš„æ£€æµ‹å¤±è´¥ï¼Œå°è¯•ç›´æ¥åˆ›å»ºCUDAå¼ é‡
        try:
            # å°è¯•åœ¨CUDAä¸Šåˆ›å»ºä¸€ä¸ªå°å¼ é‡
            test_tensor = torch.tensor([1.0], device="cuda")
            del test_tensor  # æ¸…ç†
            print(f"âœ… é€šè¿‡æµ‹è¯•å¼ é‡æ£€æµ‹åˆ°CUDAè®¾å¤‡")
            print(f"âœ… å°†ä½¿ç”¨GPUè¿›è¡ŒåŠ é€Ÿå¤„ç†")
            return "cuda"
        except RuntimeError as e:
            if "CUDA" in str(e):
                print(f"âš ï¸ æ£€æµ‹åˆ°é”™è¯¯: {e}")
                print("âš ï¸ ä½ çš„PyTorchæ²¡æœ‰CUDAæ”¯æŒ")
            pass
            
        # åœ¨Windowsä¸Šï¼Œå°è¯•ä½¿ç”¨ç³»ç»Ÿå‘½ä»¤æ£€æµ‹NVIDIAæ˜¾å¡
        nvidia_detected = False
        if os.name == 'nt':  # Windowsç³»ç»Ÿ
            try:
                # ä½¿ç”¨nvidia-smiå‘½ä»¤æ£€æµ‹æ˜¾å¡
                result = os.system('nvidia-smi >nul 2>&1')
                if result == 0:
                    print(f"âœ… é€šè¿‡nvidia-smiæ£€æµ‹åˆ°NVIDIAæ˜¾å¡")
                    nvidia_detected = True
                    
                    # æ£€æŸ¥PyTorchæ˜¯å¦æ”¯æŒCUDA
                    if not torch.cuda.is_available():
                        print("âš ï¸ æ£€æµ‹åˆ°NVIDIAæ˜¾å¡ï¼Œä½†å½“å‰PyTorchç‰ˆæœ¬ä¸æ”¯æŒCUDA")
                        print("âš ï¸ è¯·æ³¨æ„: ä½ ä½¿ç”¨çš„æ˜¯Python 3.13ï¼Œç›®å‰PyTorchå®˜æ–¹å°šæœªä¸ºæ­¤ç‰ˆæœ¬æä¾›CUDAæ”¯æŒ")
                        print("âš ï¸ å»ºè®®æ–¹æ¡ˆ:")
                        print("âš ï¸ 1. é™çº§åˆ°Python 3.10æˆ–3.11ï¼Œç„¶åå®‰è£…æ”¯æŒCUDAçš„PyTorch")
                        print("âš ï¸    pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121")
                        print("âš ï¸ 2. æˆ–è€…ç»§ç»­ä½¿ç”¨CPUæ¨¡å¼ï¼ˆé€Ÿåº¦è¾ƒæ…¢ï¼‰")
                        print("âš ï¸ å°†ä½¿ç”¨CPUå¤„ç†ï¼ˆé€Ÿåº¦è¾ƒæ…¢ï¼‰")
                        return "cpu"
                    
                    # å¼ºåˆ¶è®¾ç½®CUDAå¯è§
                    os.environ['CUDA_VISIBLE_DEVICES'] = '0'
                    # é‡æ–°åˆå§‹åŒ–CUDA
                    if torch.cuda.is_available():
                        device_name = torch.cuda.get_device_name(0)
                        print(f"âœ… å·²å¯ç”¨CUDAè®¾å¤‡: {device_name}")
                        print(f"âœ… å°†ä½¿ç”¨GPUè¿›è¡ŒåŠ é€Ÿå¤„ç†")
                        return "cuda"
            except Exception:
                pass
                
        # æ‰€æœ‰æ£€æµ‹æ–¹æ³•éƒ½å¤±è´¥ï¼Œä½¿ç”¨CPU
        if nvidia_detected:
            print("âš ï¸ æ£€æµ‹åˆ°NVIDIAæ˜¾å¡ï¼Œä½†æ— æ³•å¯ç”¨CUDA")
            print("âš ï¸ è¯·ç¡®ä¿å®‰è£…äº†æ­£ç¡®çš„CUDAç‰ˆæœ¬å’Œæ”¯æŒCUDAçš„PyTorch")
            print("âš ï¸ è¿è¡Œ: pip uninstall torch torchvision torchaudio")
            print("âš ï¸ ç„¶å: pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121")
        else:
            print("âš ï¸ æœªæ£€æµ‹åˆ°CUDAè®¾å¤‡ï¼Œå°†ä½¿ç”¨CPUå¤„ç†ï¼ˆé€Ÿåº¦è¾ƒæ…¢ï¼‰")
            print("âš ï¸ å¦‚æœä½ æœ‰NVIDIAæ˜¾å¡ï¼Œè¯·ç¡®ä¿å·²å®‰è£…æ­£ç¡®çš„CUDAå’ŒPyTorchç‰ˆæœ¬")
            print("âš ï¸ æç¤º: å¯ä»¥å°è¯•è¿è¡Œ 'pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121'")
        
        return "cpu"
    except Exception as e:
        print(f"âš ï¸ CUDAæ£€æµ‹å‡ºé”™: {e}")
        print("âš ï¸ å°†ä½¿ç”¨CPUå¤„ç†ï¼ˆé€Ÿåº¦è¾ƒæ…¢ï¼‰")
        return "cpu"

# ç¡®å®šè®¾å¤‡
DEVICE = check_cuda_availability()

# === åŠ è½½æ¨¡å‹ & å¯åŠ¨ Qdrant ===
print("ğŸ” åŠ è½½æ¨¡å‹ä¸æ•°æ®åº“...")
try:
    # å…ˆåŠ è½½æ¨¡å‹ï¼Œç¡®ä¿æ¨¡å‹å®Œå…¨ä¸‹è½½
    print("æ­£åœ¨åŠ è½½æ¨¡å‹ï¼Œé¦–æ¬¡è¿è¡Œå¯èƒ½éœ€è¦ä¸‹è½½æ¨¡å‹æ–‡ä»¶...")
    model = SentenceTransformer(MODEL_NAME, device=DEVICE)
    print("âœ“ æ¨¡å‹åŠ è½½å®Œæˆ")
    
    # ç„¶ååˆå§‹åŒ–æ•°æ®åº“
    print("æ­£åœ¨åˆå§‹åŒ–å‘é‡æ•°æ®åº“...")
    client = QdrantClient(path="./qdrant_data")
    
    # æ£€æŸ¥é›†åˆæ˜¯å¦å­˜åœ¨ï¼Œä¸å­˜åœ¨åˆ™åˆ›å»º
    if not client.collection_exists(COLLECTION_NAME):
        client.create_collection(
            collection_name=COLLECTION_NAME,
            vectors_config=VectorParams(size=VECTOR_DIM, distance=Distance.COSINE),
        )
        print(f"âœ“ åˆ›å»ºé›†åˆ {COLLECTION_NAME}")
    else:
        # å¦‚æœéœ€è¦é‡å»ºé›†åˆï¼Œå–æ¶ˆä¸‹é¢çš„æ³¨é‡Š
        # client.delete_collection(collection_name=COLLECTION_NAME)
        # client.create_collection(
        #     collection_name=COLLECTION_NAME,
        #     vectors_config=VectorParams(size=VECTOR_DIM, distance=Distance.COSINE),
        # )
        print(f"âœ“ ä½¿ç”¨ç°æœ‰é›†åˆ {COLLECTION_NAME}")
    
    print("âœ“ æ•°æ®åº“åˆå§‹åŒ–å®Œæˆ")
except Exception as e:
    print(f"âŒ åˆå§‹åŒ–å¤±è´¥: {e}")
    sys.exit(1)

# === Markdown åˆ†æ®µå‡½æ•° ===
def split_markdown_chunks(markdown_text: str, chunk_size: int = CHUNK_SIZE, overlap: int = 100):
    """
    æ™ºèƒ½åˆ†å‰² Markdown æ–‡æœ¬ä¸ºå¤šä¸ªè¯­ä¹‰å—
    
    ç‰¹ç‚¹:
    1. ä¿ç•™æ–‡æ¡£ç»“æ„å’Œæ ‡é¢˜å±‚çº§
    2. æ™ºèƒ½å¤„ç†ä»£ç å—ã€åˆ—è¡¨ç­‰ç‰¹æ®Šæ ¼å¼
    3. æ·»åŠ ä¸Šä¸‹æ–‡é‡å ä»¥æé«˜æœç´¢è´¨é‡
    4. æ›´å¥½åœ°å¤„ç†ä¸­è‹±æ–‡æ··åˆæ–‡æœ¬
    5. é¿å…åœ¨ Markdown è¯­æ³•å…³é”®ä½ç½®æˆªæ–­
    6. æ­£ç¡®å¤„ç†è¿ç»­çš„ä»£ç å—
    """
    blocks = []
    current_headers = []  # å­˜å‚¨å½“å‰æ ‡é¢˜å±‚çº§
    current_content = []
    in_code_block = False
    code_fence_marker = ""  # è®°å½•ä»£ç å—çš„å›´æ æ ‡è®°ï¼ˆå¯èƒ½æ˜¯```æˆ–~~~ï¼‰
    in_list = False
    list_indent = 0  # è®°å½•åˆ—è¡¨çš„ç¼©è¿›çº§åˆ«
    lines = markdown_text.splitlines()
    
    # ç¬¬ä¸€æ­¥ï¼šæŒ‰ç…§æ–‡æ¡£ç»“æ„åˆ†å—ï¼Œç‰¹åˆ«æ³¨æ„ä»£ç å—çš„å®Œæ•´æ€§
    for i, line in enumerate(lines):
        stripped_line = line.strip()
        
        # å¤„ç†ä»£ç å— - æ”¹è¿›è¯†åˆ«æ–¹å¼ï¼Œæ”¯æŒ```å’Œ~~~ä¸¤ç§å›´æ 
        if (stripped_line.startswith("```") or stripped_line.startswith("~~~")) and not in_code_block:
            # è¿›å…¥ä»£ç å—
            in_code_block = True
            code_fence_marker = stripped_line[:3]  # è®°å½•ä½¿ç”¨çš„å›´æ ç±»å‹
            current_content.append(line)
            continue
        elif in_code_block and stripped_line.startswith(code_fence_marker):
            # é€€å‡ºä»£ç å—
            in_code_block = False
            code_fence_marker = ""
            current_content.append(line)
            continue
            
        # åœ¨ä»£ç å—å†…ï¼Œä¸åšç‰¹æ®Šå¤„ç†
        if in_code_block:
            current_content.append(line)
            continue
            
        # å¤„ç†æ ‡é¢˜ï¼Œä½†ç¡®ä¿ä¸åœ¨ä»£ç å—ä¸­é—´åˆ†å‰²
        header_match = re.match(r'^(\s*)(#{1,6})\s+(.*?)$', line)
        if header_match and not in_code_block:
            # ä¿å­˜ä¹‹å‰çš„å†…å®¹å—
            if current_content:
                header_text = " > ".join(current_headers) if current_headers else ""
                blocks.append((header_text, "\n".join(current_content).strip()))
                current_content = []
            
            # æ›´æ–°å½“å‰æ ‡é¢˜å±‚çº§
            level = len(header_match.group(2))
            title = header_match.group(3).strip()
            
            # æ ¹æ®æ ‡é¢˜çº§åˆ«è°ƒæ•´æ ‡é¢˜å±‚çº§
            current_headers = current_headers[:level-1]  # ç§»é™¤æ›´ä½çº§åˆ«çš„æ ‡é¢˜
            if level == 1:  # å¦‚æœæ˜¯ä¸€çº§æ ‡é¢˜ï¼Œæ¸…ç©ºæ‰€æœ‰æ ‡é¢˜
                current_headers = [title]
            else:
                current_headers.append(title)
                
            # å°†æ ‡é¢˜ä¹Ÿæ·»åŠ åˆ°å½“å‰å†…å®¹ä¸­ï¼Œå¢å¼ºè¯­ä¹‰è¿è´¯æ€§
            current_content.append(line)
            continue
            
        # å¤„ç†åˆ—è¡¨é¡¹ - æ”¹è¿›åˆ—è¡¨è¯†åˆ«ï¼Œæ”¯æŒå¤šçº§ç¼©è¿›
        list_match = re.match(r'^(\s*)[-*+]\s+', line) or re.match(r'^(\s*)\d+\.\s+', line)
        if list_match:
            if not in_list:
                in_list = True
                # è®°å½•åˆ—è¡¨çš„ç¼©è¿›çº§åˆ«
                list_indent = len(list_match.group(1)) if list_match.group(1) else 0
            current_content.append(line)
            continue
        elif in_list:
            # æ£€æŸ¥æ˜¯å¦ä»åœ¨åˆ—è¡¨ä¸­ - ç©ºè¡Œæˆ–è€…ç¼©è¿›ç¬¦åˆåˆ—è¡¨ç»“æ„
            if not stripped_line:
                # ç©ºè¡Œå¯èƒ½æ˜¯åˆ—è¡¨é¡¹ä¹‹é—´çš„åˆ†éš”ï¼Œä¿ç•™åœ¨åˆ—è¡¨ä¸Šä¸‹æ–‡ä¸­
                current_content.append(line)
                continue
            # æ£€æŸ¥ç¼©è¿›æ˜¯å¦ç¬¦åˆåˆ—è¡¨ç»“æ„
            indent_match = re.match(r'^(\s+)', line)
            current_indent = len(indent_match.group(1)) if indent_match else 0
            if current_indent >= list_indent:
                # ä»åœ¨åˆ—è¡¨ä¸Šä¸‹æ–‡ä¸­
                current_content.append(line)
                continue
            else:
                # åˆ—è¡¨ç»“æŸ
                in_list = False
        
        # æ·»åŠ å½“å‰è¡Œåˆ°å†…å®¹
        current_content.append(line)
    
    # å¤„ç†æœ€åä¸€ä¸ªå—
    if current_content:
        header_text = " > ".join(current_headers) if current_headers else ""
        blocks.append((header_text, "\n".join(current_content).strip()))
    
    # ç¬¬äºŒæ­¥ï¼šé¢„å¤„ç†å—ï¼Œç¡®ä¿ä»£ç å—çš„å®Œæ•´æ€§
    processed_blocks = []
    for header, content in blocks:
        # æ£€æŸ¥ä»£ç å—æ˜¯å¦å®Œæ•´ - åˆ†åˆ«æ£€æŸ¥```å’Œ~~~ä¸¤ç§å›´æ 
        backtick_count = content.count("```")
        tilde_count = content.count("~~~")
        
        if backtick_count % 2 != 0:
            # ä»£ç å—ä¸å®Œæ•´ï¼Œæ·»åŠ ç»“æŸæ ‡è®°
            content += "\n```"
        
        if tilde_count % 2 != 0:
            # ä»£ç å—ä¸å®Œæ•´ï¼Œæ·»åŠ ç»“æŸæ ‡è®°
            content += "\n~~~"
        
        processed_blocks.append((header, content))
    
    # ç¬¬ä¸‰æ­¥ï¼šå¤„ç†è¿‡é•¿çš„å†…å®¹å—ï¼Œä½¿ç”¨æ™ºèƒ½åˆ†å‰²æ–¹æ³•
    final_chunks = []
    for header, content in processed_blocks:
        if len(content) <= chunk_size:
            final_chunks.append((header, content))
        else:
            # è¯†åˆ«éœ€è¦ä¿æŠ¤çš„ Markdown å…ƒç´ 
            protected_regions = []
            
            # 1. è¯†åˆ«æ‰€æœ‰ä»£ç å—
            # ä½¿ç”¨çŠ¶æ€æœºè€Œä¸æ˜¯ç®€å•æ­£åˆ™è¡¨è¾¾å¼ï¼Œæ›´å¯é åœ°å¤„ç†åµŒå¥—å’Œç‰¹æ®Šæƒ…å†µ
            i = 0
            while i < len(content):
                # æŸ¥æ‰¾ä»£ç å—å¼€å§‹
                if content[i:i+3] == "```" or content[i:i+3] == "~~~":
                    fence_type = content[i:i+3]
                    start = i
                    # æŸ¥æ‰¾å¯¹åº”çš„ç»“æŸæ ‡è®°
                    i += 3
                    while i < len(content):
                        if content[i:i+3] == fence_type and (i+3 >= len(content) or content[i+3] in ['\n', ' ', '\t']):
                            # æ‰¾åˆ°ç»“æŸæ ‡è®°
                            end = i + 3
                            protected_regions.append((start, end))
                            i = end
                            break
                        i += 1
                    if i >= len(content):  # æ²¡æ‰¾åˆ°ç»“æŸæ ‡è®°
                        protected_regions.append((start, len(content)))
                else:
                    i += 1
            
            # 2. è¯†åˆ«å…¶ä»– Markdown è¯­æ³•å…ƒç´ 
            # é“¾æ¥å’Œå›¾ç‰‡
            for match in re.finditer(r'\[(?:[^\[\]]|\[(?:[^\[\]]|\[[^\[\]]*\])*\])*\]\([^()]*(?:\([^()]*\)[^()]*)*\)', content):
                protected_regions.append((match.start(), match.end()))
            
            # è¡Œå†…ä»£ç 
            for match in re.finditer(r'`[^`]+`', content):
                protected_regions.append((match.start(), match.end()))
            
            # å¼ºè°ƒå’ŒåŠ ç²—
            for pattern in [r'\*\*[^*]+\*\*', r'\*[^*]+\*', r'__[^_]+__', r'_[^_]+_']:
                for match in re.finditer(pattern, content):
                    protected_regions.append((match.start(), match.end()))
            
            # è¡¨æ ¼
            table_start = -1
            for i, line in enumerate(content.split('\n')):
                if re.match(r'\s*\|.*\|\s*$', line):
                    if table_start == -1:
                        table_start = i
                elif table_start != -1:
                    # è¡¨æ ¼ç»“æŸ
                    table_end = i
                    # è®¡ç®—è¡¨æ ¼åœ¨åŸæ–‡ä¸­çš„ä½ç½®
                    start_pos = content.find('\n') * table_start if table_start > 0 else 0
                    end_pos = content.find('\n', start_pos) * table_end if table_end > 0 else len(content)
                    protected_regions.append((start_pos, end_pos))
                    table_start = -1
            
            # åˆå¹¶é‡å çš„ä¿æŠ¤åŒºåŸŸ
            if protected_regions:
                protected_regions.sort()
                merged_regions = [protected_regions[0]]
                for current in protected_regions[1:]:
                    prev = merged_regions[-1]
                    if current[0] <= prev[1]:
                        # åŒºåŸŸé‡å ï¼Œåˆå¹¶
                        merged_regions[-1] = (prev[0], max(prev[1], current[1]))
                    else:
                        merged_regions.append(current)
                protected_regions = merged_regions
            
            # æŒ‰è¯­ä¹‰å•å…ƒåˆ†å‰²å†…å®¹
            chunks = []
            last_pos = 0
            
            # é¦–å…ˆæŒ‰æ®µè½åˆ†å‰²
            paragraphs = []
            lines = content.split('\n')
            current_para = []
            
            for line in lines:
                if not line.strip() and current_para:
                    # ç©ºè¡Œæ ‡å¿—æ®µè½ç»“æŸ
                    paragraphs.append('\n'.join(current_para))
                    current_para = []
                else:
                    current_para.append(line)
            
            # æ·»åŠ æœ€åä¸€ä¸ªæ®µè½
            if current_para:
                paragraphs.append('\n'.join(current_para))
            
            # æ ¹æ®æ®µè½å’Œä¿æŠ¤åŒºåŸŸæ„å»ºå—
            current_chunk = ""
            for para in paragraphs:
                # æ£€æŸ¥æ®µè½æ˜¯å¦ä¸ä»»ä½•ä¿æŠ¤åŒºåŸŸé‡å 
                para_start = content.find(para, last_pos)
                para_end = para_start + len(para)
                last_pos = para_end
                
                # æ£€æŸ¥æ®µè½æ˜¯å¦éœ€è¦ä¿æŒå®Œæ•´
                para_protected = False
                for start, end in protected_regions:
                    if (start < para_end and end > para_start):
                        para_protected = True
                        break
                
                # å¦‚æœæ·»åŠ è¿™ä¸ªæ®µè½ä¼šå¯¼è‡´å—è¿‡å¤§ï¼Œå¹¶ä¸”å½“å‰å—ä¸ä¸ºç©ºï¼Œåˆ™å¼€å§‹æ–°å—
                if len(current_chunk) + len(para) + 2 > chunk_size and current_chunk and not para_protected:
                    # ç¡®ä¿å½“å‰å—çš„è¯­æ³•å®Œæ•´æ€§
                    if not is_markdown_syntax_complete(current_chunk):
                        current_chunk = fix_markdown_syntax(current_chunk)
                    chunks.append(current_chunk)
                    current_chunk = para
                else:
                    # æ·»åŠ æ®µè½åˆ°å½“å‰å—
                    if current_chunk:
                        current_chunk += "\n\n" + para
                    else:
                        current_chunk = para
                
                # ç‰¹æ®Šæƒ…å†µï¼šå¦‚æœå½“å‰æ®µè½æœ¬èº«å°±è¶…è¿‡äº†å—å¤§å°ï¼Œéœ€è¦è¿›ä¸€æ­¥åˆ†å‰²
                if len(current_chunk) > chunk_size and not para_protected:
                    # å°è¯•åœ¨å¥å­è¾¹ç•Œåˆ†å‰²
                    sentences = re.split(r'(?<=[.!?])\s+', current_chunk)
                    current_chunk = ""
                    for sentence in sentences:
                        if len(current_chunk) + len(sentence) + 1 > chunk_size and current_chunk:
                            # ç¡®ä¿å½“å‰å—çš„è¯­æ³•å®Œæ•´æ€§
                            if not is_markdown_syntax_complete(current_chunk):
                                current_chunk = fix_markdown_syntax(current_chunk)
                            chunks.append(current_chunk)
                            current_chunk = sentence
                        else:
                            if current_chunk:
                                current_chunk += " " + sentence
                            else:
                                current_chunk = sentence
            
            # æ·»åŠ æœ€åä¸€ä¸ªå—
            if current_chunk:
                # ç¡®ä¿è¯­æ³•å®Œæ•´æ€§
                if not is_markdown_syntax_complete(current_chunk):
                    current_chunk = fix_markdown_syntax(current_chunk)
                chunks.append(current_chunk)
            
            # å¦‚æœæ²¡æœ‰æˆåŠŸåˆ†å‰²ï¼Œä½¿ç”¨åŸå§‹å†…å®¹
            if not chunks:
                chunks = [content]
            
            # å°†åˆ†å‰²åçš„å—æ·»åŠ åˆ°æœ€ç»ˆç»“æœ
            for chunk in chunks:
                final_chunks.append((header, chunk))
    
    # ç¬¬å››æ­¥ï¼šæ·»åŠ ä¸Šä¸‹æ–‡é‡å ï¼ˆå¯é€‰ï¼‰
    if overlap > 0 and len(final_chunks) > 1:
        overlapped_chunks = []
        for i, (header, content) in enumerate(final_chunks):
            if i > 0:
                # ä»å‰ä¸€ä¸ªå—çš„æœ«å°¾æ·»åŠ é‡å å†…å®¹
                prev_content = final_chunks[i-1][1]
                # æ™ºèƒ½é€‰æ‹©é‡å å†…å®¹ï¼šå°½é‡åœ¨å¥å­è¾¹ç•Œå¤„æˆªæ–­
                overlap_text = get_smart_overlap(prev_content, overlap, is_end=True)
                content = overlap_text + "..." + content
            
            if i < len(final_chunks) - 1:
                # ä»åä¸€ä¸ªå—çš„å¼€å¤´æ·»åŠ é‡å å†…å®¹
                next_content = final_chunks[i+1][1]
                # æ™ºèƒ½é€‰æ‹©é‡å å†…å®¹ï¼šå°½é‡åœ¨å¥å­è¾¹ç•Œå¤„æˆªæ–­
                overlap_text = get_smart_overlap(next_content, overlap, is_end=False)
                content = content + "..." + overlap_text
                
            overlapped_chunks.append((header, content))
        return overlapped_chunks
    
    return final_chunks

def get_smart_overlap(text, max_length, is_end=False):
    """
    æ™ºèƒ½é€‰æ‹©é‡å å†…å®¹ï¼Œå°½é‡åœ¨å¥å­è¾¹ç•Œå¤„æˆªæ–­
    
    å‚æ•°:
    - text: è¦å¤„ç†çš„æ–‡æœ¬
    - max_length: æœ€å¤§é‡å é•¿åº¦
    - is_end: æ˜¯å¦ä»æ–‡æœ¬æœ«å°¾é€‰æ‹©
    
    è¿”å›:
    - é€‰æ‹©çš„é‡å æ–‡æœ¬
    """
    if len(text) <= max_length:
        return text
    
    if is_end:
        # ä»æœ«å°¾é€‰æ‹©
        text_portion = text[-max_length*2:]  # é€‰æ‹©æ›´é•¿çš„éƒ¨åˆ†æ¥å¯»æ‰¾åˆé€‚çš„è¾¹ç•Œ
        # å°è¯•åœ¨å¥å­è¾¹ç•Œå¤„åˆ†å‰²
        sentences = re.split(r'(?<=[.!?])\s+', text_portion)
        result = ""
        for sentence in reversed(sentences):
            if len(result) + len(sentence) + 1 <= max_length or not result:
                result = sentence + " " + result if result else sentence
            else:
                break
        return result.strip()
    else:
        # ä»å¼€å¤´é€‰æ‹©
        text_portion = text[:max_length*2]  # é€‰æ‹©æ›´é•¿çš„éƒ¨åˆ†æ¥å¯»æ‰¾åˆé€‚çš„è¾¹ç•Œ
        # å°è¯•åœ¨å¥å­è¾¹ç•Œå¤„åˆ†å‰²
        sentences = re.split(r'(?<=[.!?])\s+', text_portion)
        result = ""
        for sentence in sentences:
            if len(result) + len(sentence) + 1 <= max_length or not result:
                result = result + " " + sentence if result else sentence
            else:
                break
        return result.strip()

def is_markdown_syntax_complete(text):
    """æ£€æŸ¥ Markdown è¯­æ³•æ˜¯å¦å®Œæ•´"""
    # æ£€æŸ¥ä»£ç å—
    if text.count("```") % 2 != 0:
        return False
    
    # æ£€æŸ¥é“¾æ¥å’Œå›¾ç‰‡
    if text.count("[") != text.count("]") or text.count("(") != text.count(")"):
        return False
    
    # æ£€æŸ¥å¼ºè°ƒæ ‡è®°
    if (text.count("**") % 2 != 0 or 
        text.count("*") % 2 != 0 or
        text.count("__") % 2 != 0 or
        text.count("_") % 2 != 0):
        return False
    
    return True

def fix_markdown_syntax(text):
    """ä¿®å¤ Markdown è¯­æ³•ä¸­å¯èƒ½çš„æˆªæ–­é—®é¢˜"""
    # ä¿®å¤ä»£ç å—
    if text.count("```") % 2 != 0:
        text += "\n```"
    
    # ä¿®å¤é“¾æ¥å’Œå›¾ç‰‡
    if text.count("[") > text.count("]"):
        text += "]"
    if text.count("(") > text.count(")"):
        text += ")"
    
    # ä¿®å¤å¼ºè°ƒæ ‡è®°
    if text.count("**") % 2 != 0:
        text += "**"
    if text.count("*") % 2 != 0:
        text += "*"
    if text.count("__") % 2 != 0:
        text += "__"
    if text.count("_") % 2 != 0:
        text += "_"
    
    return text

# === éå†ç¬”è®°å¹¶å†™å…¥å‘é‡æ•°æ®åº“ ===
all_points = []
file_count = 0

print("ğŸ“ æ­£åœ¨æ‰«æå¹¶å¤„ç† Markdown æ–‡ä»¶...")
for path in tqdm(list(ROOT_DIR.rglob("*"))):
    if not path.is_file() or path.suffix.lower() not in EXTENSIONS:
        continue

    try:
        text = path.read_text(encoding="utf-8")
    except Exception:
        continue

    # æå–æ–‡ä»¶åï¼ˆä¸å«æ‰©å±•åï¼‰ä½œä¸ºé¢å¤–çš„æ–‡æœ¬å†…å®¹
    filename = path.stem
    
    # å¤„ç†æ–‡ä»¶åï¼Œå°†é©¼å³°å‘½åã€ä¸‹åˆ’çº¿ã€è¿å­—ç¬¦ç­‰åˆ†å‰²ä¸ºç©ºæ ¼
    # ä¾‹å¦‚: "Windowså¼€å¯bbr" -> "Windows å¼€å¯ bbr"
    # ä¾‹å¦‚: "windows_bbr_config" -> "windows bbr config"
    processed_filename = re.sub(r'([a-z])([A-Z])', r'\1 \2', filename)  # é©¼å³°è½¬ç©ºæ ¼
    processed_filename = re.sub(r'[_\-.]', ' ', processed_filename)     # ä¸‹åˆ’çº¿ã€è¿å­—ç¬¦è½¬ç©ºæ ¼
    
    # å°†æ–‡ä»¶åæ·»åŠ åˆ°æ–‡æœ¬å†…å®¹çš„å¼€å¤´ï¼Œå¢åŠ æƒé‡
    text_with_filename = f"# {processed_filename}\n\n{text}"
    
    chunks = split_markdown_chunks(text_with_filename)
    if not chunks:
        continue

    sentences = [f"{title}\n{content}" if title else content for title, content in chunks]
    vectors = model.encode(sentences, show_progress_bar=False)

    for i, vec in enumerate(vectors):
        # ä½¿ç”¨ uuid5 åŸºäº SHA1 å“ˆå¸Œç”Ÿæˆç¡®å®šæ€§ UUID
        namespace = uuid.NAMESPACE_DNS
        uid = uuid.uuid5(namespace, f"{path}-{i}")
        all_points.append(PointStruct(
            id=str(uid),  # å°† UUID è½¬æ¢ä¸ºå­—ç¬¦ä¸²
            vector=vec.tolist(),
            payload={
                "text": sentences[i],
                "source": str(path),
                "filename": path.name  # æ·»åŠ æ–‡ä»¶ååˆ°payload
            }
        ))

    # é¢å¤–æ·»åŠ ä¸€ä¸ªä»…åŒ…å«æ–‡ä»¶åçš„å‘é‡ç‚¹ï¼Œæé«˜æ–‡ä»¶åæœç´¢çš„å‡†ç¡®æ€§
    filename_vector = model.encode(processed_filename)
    filename_uid = uuid.uuid5(namespace, f"{path}-filename")
    all_points.append(PointStruct(
        id=str(filename_uid),
        vector=filename_vector.tolist(),
        payload={
            "text": f"# {processed_filename}",
            "source": str(path),
            "filename": path.name,
            "is_filename_only": True  # æ ‡è®°è¿™æ˜¯ä¸€ä¸ªä»…åŒ…å«æ–‡ä»¶åçš„å‘é‡ç‚¹
        }
    ))

    file_count += 1
    if len(all_points) >= 128:
        client.upsert(collection_name=COLLECTION_NAME, points=all_points)
        all_points = []

if all_points:
    client.upsert(collection_name=COLLECTION_NAME, points=all_points)

print(f"âœ… å®Œæˆï¼šå…±å¤„ç† {file_count} ä¸ªæ–‡ä»¶ï¼Œå‘é‡æ•°æ®å·²å†™å…¥ Qdrant æ•°æ®åº“ã€‚")