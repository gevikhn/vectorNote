import streamlit as st
import subprocess
import json

# å¿…é¡»æ˜¯ç¬¬ä¸€ä¸ª Streamlit å‘½ä»¤
st.set_page_config(page_title="Obsidian æœç´¢", layout="wide")

from sentence_transformers import SentenceTransformer
from qdrant_client import QdrantClient
import re
import urllib.parse
from pathlib import Path
import torch
import os
import streamlit.components.v1 as components
import hashlib

# é…ç½®
COLLECTION_NAME = "obsidian_notes"
MODEL_NAME = "BAAI/bge-large-zh-noinstruct"  # å‡çº§åˆ°æ›´å¼ºå¤§çš„æ¨¡å‹
VAULT_ROOT = "D:/Notes"  # â† ä¿®æ”¹ä¸ºä½ æœ¬åœ°ç¬”è®°åº“è·¯å¾„

# åº”ç”¨ç¨‹åºæ‰“å¼€å‡½æ•°
def open_file_with_app(file_path):
    """ä½¿ç”¨ç³»ç»Ÿé»˜è®¤åº”ç”¨ç¨‹åºæ‰“å¼€æ–‡ä»¶"""
    try:
        # ä½¿ç”¨ç³»ç»Ÿé»˜è®¤åº”ç”¨æ‰“å¼€
        if os.name == 'nt':  # Windows
            os.startfile(file_path)
        else:  # macOS å’Œ Linux
            subprocess.run(["xdg-open", file_path], check=True)
        return True, ""
    except Exception as e:
        return False, str(e)

# === æ£€æµ‹CUDAå¯ç”¨æ€§ ===
def check_cuda_availability():
    """æ£€æµ‹æ˜¯å¦æœ‰å¯ç”¨çš„CUDAè®¾å¤‡ï¼Œç‰¹åˆ«é’ˆå¯¹Windowsç¯å¢ƒä¼˜åŒ–"""
    try:
        # å°è¯•ç›´æ¥è·å–CUDAè®¾å¤‡ä¿¡æ¯
        if torch.cuda.is_available():
            device_count = torch.cuda.device_count()
            device_name = torch.cuda.get_device_name(0) if device_count > 0 else "æœªçŸ¥"
            st.sidebar.success(f"âœ… æ£€æµ‹åˆ° {device_count} ä¸ªCUDAè®¾å¤‡: {device_name}")
            st.sidebar.success(f"âœ… å°†ä½¿ç”¨GPUè¿›è¡ŒåŠ é€Ÿå¤„ç†")
            return "cuda"
        
        # å¦‚æœä¸Šé¢çš„æ£€æµ‹å¤±è´¥ï¼Œå°è¯•ç›´æ¥åˆ›å»ºCUDAå¼ é‡
        try:
            # å°è¯•åœ¨CUDAä¸Šåˆ›å»ºä¸€ä¸ªå°å¼ é‡
            test_tensor = torch.tensor([1.0], device="cuda")
            del test_tensor  # æ¸…ç†
            st.sidebar.success(f"âœ… é€šè¿‡æµ‹è¯•å¼ é‡æ£€æµ‹åˆ°CUDAè®¾å¤‡")
            st.sidebar.success(f"âœ… å°†ä½¿ç”¨GPUè¿›è¡ŒåŠ é€Ÿå¤„ç†")
            return "cuda"
        except RuntimeError as e:
            if "CUDA" in str(e):
                st.sidebar.warning(f"âš ï¸ æ£€æµ‹åˆ°é”™è¯¯: {e}")
                st.sidebar.warning("âš ï¸ ä½ çš„PyTorchæ²¡æœ‰CUDAæ”¯æŒ")
            pass
            
        # åœ¨Windowsä¸Šï¼Œå°è¯•ä½¿ç”¨ç³»ç»Ÿå‘½ä»¤æ£€æµ‹NVIDIAæ˜¾å¡
        nvidia_detected = False
        if os.name == 'nt':  # Windowsç³»ç»Ÿ
            try:
                # ä½¿ç”¨nvidia-smiå‘½ä»¤æ£€æµ‹æ˜¾å¡
                result = os.system('nvidia-smi >nul 2>&1')
                if result == 0:
                    st.sidebar.success(f"âœ… é€šè¿‡nvidia-smiæ£€æµ‹åˆ°NVIDIAæ˜¾å¡")
                    nvidia_detected = True
                    
                    # æ£€æŸ¥PyTorchæ˜¯å¦æ”¯æŒCUDA
                    if not torch.cuda.is_available():
                        st.sidebar.warning("âš ï¸ æ£€æµ‹åˆ°NVIDIAæ˜¾å¡ï¼Œä½†å½“å‰PyTorchç‰ˆæœ¬ä¸æ”¯æŒCUDA")
                        st.sidebar.warning("âš ï¸ è¯·æ³¨æ„: ä½ ä½¿ç”¨çš„æ˜¯Python 3.13ï¼Œç›®å‰PyTorchå®˜æ–¹å°šæœªä¸ºæ­¤ç‰ˆæœ¬æä¾›CUDAæ”¯æŒ")
                        st.sidebar.warning("âš ï¸ å»ºè®®æ–¹æ¡ˆ:")
                        st.sidebar.warning("âš ï¸ 1. é™çº§åˆ°Python 3.10æˆ–3.11ï¼Œç„¶åå®‰è£…æ”¯æŒCUDAçš„PyTorch")
                        st.sidebar.warning("âš ï¸    pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121")
                        st.sidebar.warning("âš ï¸ 2. æˆ–è€…ç»§ç»­ä½¿ç”¨CPUæ¨¡å¼ï¼ˆé€Ÿåº¦è¾ƒæ…¢ï¼‰")
                        st.sidebar.warning("âš ï¸ å°†ä½¿ç”¨CPUå¤„ç†ï¼ˆé€Ÿåº¦è¾ƒæ…¢ï¼‰")
                        return "cpu"
                    
                    # å¼ºåˆ¶è®¾ç½®CUDAå¯è§
                    os.environ['CUDA_VISIBLE_DEVICES'] = '0'
                    # é‡æ–°åˆå§‹åŒ–CUDA
                    if torch.cuda.is_available():
                        device_name = torch.cuda.get_device_name(0)
                        st.sidebar.success(f"âœ… å·²å¯ç”¨CUDAè®¾å¤‡: {device_name}")
                        st.sidebar.success(f"âœ… å°†ä½¿ç”¨GPUè¿›è¡ŒåŠ é€Ÿå¤„ç†")
                        return "cuda"
            except Exception:
                pass
                
        # æ‰€æœ‰æ£€æµ‹æ–¹æ³•éƒ½å¤±è´¥ï¼Œä½¿ç”¨CPU
        if nvidia_detected:
            st.sidebar.warning("âš ï¸ æ£€æµ‹åˆ°NVIDIAæ˜¾å¡ï¼Œä½†æ— æ³•å¯ç”¨CUDA")
            st.sidebar.warning("âš ï¸ è¯·ç¡®ä¿å®‰è£…äº†æ­£ç¡®çš„CUDAç‰ˆæœ¬å’Œæ”¯æŒCUDAçš„PyTorch")
            st.sidebar.warning("âš ï¸ è¿è¡Œ: pip uninstall torch torchvision torchaudio")
            st.sidebar.warning("âš ï¸ ç„¶å: pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121")
        else:
            st.sidebar.warning("âš ï¸ æœªæ£€æµ‹åˆ°CUDAè®¾å¤‡ï¼Œå°†ä½¿ç”¨CPUå¤„ç†ï¼ˆé€Ÿåº¦è¾ƒæ…¢ï¼‰")
            st.sidebar.warning("âš ï¸ å¦‚æœä½ æœ‰NVIDIAæ˜¾å¡ï¼Œè¯·ç¡®ä¿å·²å®‰è£…æ­£ç¡®çš„CUDAå’ŒPyTorchç‰ˆæœ¬")
            st.sidebar.warning("âš ï¸ æç¤º: å¯ä»¥å°è¯•è¿è¡Œ 'pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121'")
        
        return "cpu"
    except Exception as e:
        st.sidebar.warning(f"âš ï¸ CUDAæ£€æµ‹å‡ºé”™: {e}")
        st.sidebar.warning("âš ï¸ å°†ä½¿ç”¨CPUå¤„ç†ï¼ˆé€Ÿåº¦è¾ƒæ…¢ï¼‰")
        return "cpu"

# ç¡®å®šè®¾å¤‡
DEVICE = check_cuda_availability()

# åˆå§‹åŒ–
@st.cache_resource
def load_model_and_client():
    """åŠ è½½æ¨¡å‹å’Œæ•°æ®åº“å®¢æˆ·ç«¯"""
    try:
        model = SentenceTransformer(MODEL_NAME, device=DEVICE)
        client = QdrantClient(path="./qdrant_data")
        return model, client
    except Exception as e:
        st.error(f"åŠ è½½æ¨¡å‹æˆ–æ•°æ®åº“æ—¶å‡ºé”™: {str(e)}")
        st.info("å°è¯•é‡æ–°åˆå§‹åŒ–æ•°æ®åº“...")
        try:
            # å°è¯•é‡æ–°åˆ›å»ºæ•°æ®åº“è¿æ¥
            client = QdrantClient(":memory:")  # ä¸´æ—¶ä½¿ç”¨å†…å­˜æ•°æ®åº“
            st.warning("âš ï¸ ä½¿ç”¨ä¸´æ—¶å†…å­˜æ•°æ®åº“ã€‚è¯·å…ˆè¿è¡Œ scan_and_embed_notes.py é‡å»ºç´¢å¼•ã€‚")
            model = SentenceTransformer(MODEL_NAME, device=DEVICE)
            return model, client
        except Exception as e2:
            st.error(f"æ— æ³•åˆ›å»ºä¸´æ—¶æ•°æ®åº“: {str(e2)}")
            # è¿”å›Noneï¼Œåç»­ä»£ç éœ€è¦å¤„ç†Noneçš„æƒ…å†µ
            return None, None

# åŠ è½½æ¨¡å‹å’Œå®¢æˆ·ç«¯
model, client = load_model_and_client()

# æ£€æŸ¥æ¨¡å‹å’Œå®¢æˆ·ç«¯æ˜¯å¦æˆåŠŸåŠ è½½
if model is None or client is None:
    st.error("æ— æ³•åŠ è½½æ¨¡å‹æˆ–æ•°æ®åº“ï¼Œè¯·æ£€æŸ¥é”™è¯¯ä¿¡æ¯å¹¶é‡è¯•ã€‚")
    st.stop()

# UI
st.title("ğŸ” Obsidian ç¬”è®°è¯­ä¹‰æœç´¢")

query = st.text_input("è¯·è¾“å…¥ä½ çš„é—®é¢˜æˆ–å…³é”®è¯ï¼š", "")
top_k = st.slider("è¿”å›ç»“æœæ•°é‡", 1, 20, 5)

# æŸ¥è¯¢å¢å¼ºå‡½æ•°
def enhance_query(query: str):
    """
    å¢å¼ºæŸ¥è¯¢æ–‡æœ¬ï¼Œæé«˜æ£€ç´¢æ•ˆæœ
    """
    # 1. å»é™¤å¤šä½™ç©ºæ ¼å’Œæ ‡ç‚¹
    query = re.sub(r'\s+', ' ', query).strip()
    
    # 2. æ·»åŠ æŸ¥è¯¢å‰ç¼€ï¼Œæé«˜æ£€ç´¢è´¨é‡ï¼ˆBGEæ¨¡å‹ç‰¹æ€§ï¼‰
    enhanced_query = f"æŸ¥è¯¢ï¼š{query}"
    
    return enhanced_query

# æœç´¢é€»è¾‘
if query:
    # åº”ç”¨æŸ¥è¯¢å¢å¼º
    enhanced_query = enhance_query(query)
    
    # å°†æŸ¥è¯¢æ–‡æœ¬è½¬æ¢ä¸ºå‘é‡
    query_vector = model.encode(enhanced_query).tolist()
    
    with st.spinner("æ­£åœ¨æœç´¢..."):
        # è·å–æ›´å¤šç»“æœï¼Œåé¢ä¼šé‡æ’åº
        results = client.query_points(
            collection_name=COLLECTION_NAME,
            query=query_vector,
            limit=top_k * 3,
            score_threshold=0.45  # é™ä½ç›¸ä¼¼åº¦é˜ˆå€¼ï¼Œå¢åŠ å¬å›ç‡
        ).points
        
        # æ–‡ä»¶åç²¾ç¡®åŒ¹é…æœç´¢ï¼ˆä¼˜å…ˆæ˜¾ç¤ºï¼‰
        file_matches = []
        query_terms = query.lower().split()
        
        # æ ¹æ®æ–‡ä»¶è·¯å¾„å’Œæ–‡ä»¶åè¿›è¡ŒåŒ¹é…
        for result in results:
            # æ£€æŸ¥æ˜¯å¦æ˜¯æ–‡ä»¶åå‘é‡ç‚¹
            is_filename_only = result.payload.get("is_filename_only", False)
            
            # è·å–æ–‡ä»¶å
            filename = result.payload.get("filename", "")
            if not filename:
                source_path = Path(result.payload["source"])
                filename = source_path.name
                
            filename_lower = filename.lower()
            
            # æ–‡ä»¶åå‘é‡ç‚¹ä¼˜å…ˆçº§æ›´é«˜
            if is_filename_only and all(term in filename_lower for term in query_terms):
                file_matches.insert(0, result)  # æ’å…¥åˆ°æœ€å‰é¢
            # æ™®é€šå‘é‡ç‚¹ä½†æ–‡ä»¶ååŒ¹é…
            elif all(term in filename_lower for term in query_terms):
                file_matches.append(result)
        
        # é‡æ’åºç»“æœï¼šç»“åˆç›¸ä¼¼åº¦åˆ†æ•°å’Œå…³é”®è¯åŒ¹é…åº¦
        def rerank_score(result):
            base_score = result.score
            text = result.payload["text"].lower()
            
            # è®¡ç®—å…³é”®è¯åŒ¹é…åº¦
            keyword_bonus = 0
            for term in query_terms:
                if term in text:
                    # æ ¹æ®å…³é”®è¯å‡ºç°çš„ä½ç½®ç»™äºˆä¸åŒæƒé‡
                    # æ ‡é¢˜ä¸­å‡ºç°çš„å…³é”®è¯æƒé‡æ›´é«˜
                    if term in text.split('\n')[0]:
                        keyword_bonus += 0.1
                    else:
                        keyword_bonus += 0.05
            
            # æ–‡ä»¶ååŒ¹é…åŠ åˆ†
            filename_bonus = 0
            filename = result.payload.get("filename", "").lower()
            if any(term in filename for term in query_terms):
                filename_bonus = 0.15
            
            # æ˜¯å¦ä¸ºæ–‡ä»¶åå‘é‡ç‚¹
            is_filename_only = result.payload.get("is_filename_only", False)
            filename_only_bonus = 0.2 if is_filename_only and any(term in filename for term in query_terms) else 0
            
            # æœ€ç»ˆåˆ†æ•°
            final_score = base_score + keyword_bonus + filename_bonus + filename_only_bonus
            return final_score
        
        # åˆå¹¶ç»“æœ
        combined_results = file_matches + [r for r in results if r not in file_matches]
        
        # æ ¹æ®é‡æ’åºåˆ†æ•°æ’åº
        combined_results.sort(key=rerank_score, reverse=True)
        
        # å»é‡å¹¶é™åˆ¶ç»“æœæ•°é‡
        unique_results = []
        unique_paths = set()
        
        for result in combined_results:
            source = result.payload["source"]
            if source not in unique_paths and len(unique_results) < top_k:
                unique_paths.add(source)
                unique_results.append(result)
        
        # ä½¿ç”¨é‡æ’åºåçš„ç»“æœ
        results = unique_results

    if results:
        st.subheader("ğŸ“„ åŒ¹é…ç»“æœï¼š")

        keywords = list(set(re.findall(r'\w+', query.lower())))

        for hit in results:
            raw_path = hit.payload["source"]
            content = hit.payload["text"]
            
            # å°è¯•è¯»å–åŸå§‹æ–‡ä»¶ä»¥è·å–æ›´å®Œæ•´çš„å†…å®¹
            try:
                if os.path.exists(raw_path):
                    with open(raw_path, 'r', encoding='utf-8') as f:
                        full_content = f.read()
                    
                    # æå–æ–‡ä»¶çš„ä¸»è¦å†…å®¹ï¼ˆæœ€å¤šæ˜¾ç¤ºå‰50è¡Œæœ‰æ„ä¹‰çš„å†…å®¹ï¼‰
                    lines = full_content.split('\n')
                    # å»é™¤ç©ºè¡Œ
                    meaningful_lines = [line for line in lines if line.strip()]
                    
                    # æå–å‰50è¡Œéç©ºå†…å®¹
                    if len(meaningful_lines) > 50:
                        preview_lines = meaningful_lines[:50]
                        preview_text = '\n'.join(preview_lines)
                        preview_text += "\n...(æ›´å¤šå†…å®¹)"
                    else:
                        preview_text = full_content
                    
                    # ä½¿ç”¨å®Œæ•´å†…å®¹æ›¿æ¢å‘é‡æ•°æ®åº“ä¸­çš„ç‰‡æ®µ
                    content = preview_text
            except Exception as e:
                st.warning(f"è¯»å–åŸå§‹æ–‡ä»¶æ—¶å‡ºé”™: {str(e)}ï¼Œå°†ä½¿ç”¨å‘é‡æ•°æ®åº“ä¸­çš„å†…å®¹ç‰‡æ®µ")
            
            # æ£€æŸ¥å¹¶ä¿®å¤å¯èƒ½çš„Markdownæˆªæ–­é—®é¢˜
            def fix_truncated_markdown(text):
                # ä¿®å¤å¯èƒ½è¢«æˆªæ–­çš„å›¾ç‰‡é“¾æ¥
                img_pattern = r'!\[.*?\]\([^\)]*$'
                if re.search(img_pattern, text):
                    text += ")"  # æ·»åŠ ç¼ºå¤±çš„å³æ‹¬å·
                
                # ä¿®å¤å¯èƒ½è¢«æˆªæ–­çš„é“¾æ¥
                link_pattern = r'\[.*?\]\([^\)]*$'
                if re.search(link_pattern, text):
                    text += ")"  # æ·»åŠ ç¼ºå¤±çš„å³æ‹¬å·
                
                # ä¿®å¤å¯èƒ½è¢«æˆªæ–­çš„ä»£ç å—
                if text.count("```") % 2 != 0:
                    text += "\n```"  # æ·»åŠ ç¼ºå¤±çš„ä»£ç å—ç»“æŸæ ‡è®°
                
                # ä¿®å¤å¯èƒ½è¢«æˆªæ–­çš„å¼ºè°ƒæ ‡è®°
                if text.count("**") % 2 != 0:
                    text += "**"  # æ·»åŠ ç¼ºå¤±çš„å¼ºè°ƒç»“æŸæ ‡è®°
                
                if text.count("*") % 2 != 0:
                    text += "*"  # æ·»åŠ ç¼ºå¤±çš„æ–œä½“ç»“æŸæ ‡è®°
                
                if text.count("__") % 2 != 0:
                    text += "__"  # æ·»åŠ ç¼ºå¤±çš„ä¸‹åˆ’çº¿ç»“æŸæ ‡è®°
                
                if text.count("_") % 2 != 0:
                    text += "_"  # æ·»åŠ ç¼ºå¤±çš„ä¸‹åˆ’çº¿ç»“æŸæ ‡è®°
                
                return text
            
            # ä¿®å¤å¯èƒ½çš„æˆªæ–­é—®é¢˜
            content = fix_truncated_markdown(content)
            
            # é«˜äº®å…³é”®è¯
            highlighted_content = content
            for word in keywords:
                if len(word) >= 2:
                    highlighted_content = re.sub(
                        fr'\b({re.escape(word)})\b', 
                        r'<span style="background-color: yellow; font-weight: bold;">\1</span>', 
                        highlighted_content, 
                        flags=re.IGNORECASE
                    )
            
            # æ–‡æ¡£è·³è½¬é“¾æ¥
            abs_path = Path(raw_path).resolve()

            # æ·»åŠ æ–‡ä»¶è·¯å¾„å’Œæ‰“å¼€æŒ‰é’®
            col1, col2 = st.columns([4, 1])
            with col1:
                st.markdown(f"**ğŸ“ æ–‡ä»¶è·¯å¾„ï¼š** {raw_path}", unsafe_allow_html=True)
            with col2:
                if st.button("ğŸ”— æ‰“å¼€æ–‡ä»¶", key=f"link_{raw_path}"):
                    # ä½¿ç”¨ç³»ç»Ÿé»˜è®¤æ–¹å¼æ‰“å¼€æ–‡ä»¶
                    success, error = open_file_with_app(str(abs_path))
                    if not success:
                        st.error(f"æ‰“å¼€å¤±è´¥: {error}")
            
            # ä½¿ç”¨Streamlitçš„expanderç»„ä»¶æ˜¾ç¤ºå†…å®¹
            with st.expander("ğŸ“ ç¬”è®°å†…å®¹", expanded=True):
                # æ·»åŠ è‡ªå®šä¹‰CSSæ ·å¼
                st.markdown("""
                <style>
                .markdown-content img {
                    max-width: 100%;
                    height: auto;
                }
                .markdown-content pre {
                    background-color: #f6f8fa;
                    border-radius: 3px;
                    padding: 16px;
                    overflow: auto;
                }
                .markdown-content code {
                    font-family: SFMono-Regular, Consolas, Liberation Mono, Menlo, monospace;
                    font-size: 85%;
                    padding: 0.2em 0.4em;
                    background-color: rgba(27, 31, 35, 0.05);
                    border-radius: 3px;
                }
                .markdown-content pre code {
                    background-color: transparent;
                    padding: 0;
                }
                </style>
                """, unsafe_allow_html=True)
                
                # ä½¿ç”¨divåŒ…è£…å†…å®¹ä»¥åº”ç”¨æ ·å¼
                st.markdown(f'<div class="markdown-content">{highlighted_content}</div>', unsafe_allow_html=True)
            
            st.markdown(f"**ğŸ”¢ ç›¸ä¼¼åº¦ï¼š** `{round(hit.score, 4)}`", unsafe_allow_html=True)
            st.markdown("---")
    else:
        st.warning("æ²¡æœ‰æ‰¾åˆ°ç›¸å…³å†…å®¹ã€‚")